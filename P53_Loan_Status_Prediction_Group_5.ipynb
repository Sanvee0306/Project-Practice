{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4ThNvOVZSrq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6H2Qhl6ZSru"
   },
   "source": [
    "# Problem Statement Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9EzH7hhZSru"
   },
   "source": [
    "### Create a Predictive model for Loan Status\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GlpY1FNQRYY"
   },
   "source": [
    "**Target Variable** : Loan_Status                                               \n",
    "**Predictors** : Gender,Married,Dependents,Education,Self_Employed,ApplicantIncome,CoapplicantIncome,LoanAmount,Loan_Amount_Term,  Credit_History,Property_Area,Loan_Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB6VaARPZSrv"
   },
   "source": [
    "## Define the Type of Machine Learning Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbBkGfTLZSrv"
   },
   "source": [
    "Based on the problem statement you can understand that we need to create a **Supervised ML classification** model, as the target variable is categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKpvzyCOZSrv"
   },
   "source": [
    "#  **1. Data Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjrG7BjvZSrw"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "testpath = 'test.csv'\n",
    "trainpath = 'train.csv'\n",
    "test_df = pd.read_csv(testpath)\n",
    "train_df = pd.read_csv(trainpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAgS8Z2xZSrw"
   },
   "source": [
    "## 1.1  Basic Understanding of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKblsoS8ZSrw"
   },
   "source": [
    "This step is performed to guage the overall data. The volume of data, the types of columns present in the data. Initial assessment of the data should be done to identify which columns are Quantitative, Categorical or Qualitative.\n",
    "\n",
    "This step helps to start the column rejection process. You must look at each column carefully and ask, does this column affect the values of the Target variable? For example in this case study, you will ask, does this column affect the loan status? If the answer is a clear \"No\", then remove the column immediately from the data, otherwise keep the column for further analysis.\n",
    "\n",
    "There are some of the below commands which are used for Basic data exploration in Python\n",
    "\n",
    "     head(),tail() : This helps to see a few sample rows of the data\n",
    "     shape : This helps us to identify how many rows and columns present in dataset\n",
    "     info(),dtypes : This provides the summarized information of the data\n",
    "     describe() : This provides the descriptive statistical details of the data\n",
    "     nunique(): This helps us to identify if a column is categorical or continuous\n",
    "     isnull(): This helps us to identify how many are null values in a column\n",
    "     duplicated() : This helps us to identify if we have any duplicate rows present in data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frXkmy4lZSrx"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDDmbIYSZSry"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GH3pIsUXri2y"
   },
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Sg4ghGxZSrz"
   },
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ug5dXSmZSry"
   },
   "outputs": [],
   "source": [
    "train_df.shape,test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZwgmdFWZSrz"
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRq05ABGZSrz"
   },
   "outputs": [],
   "source": [
    "train_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ivuiHkUZSr0"
   },
   "outputs": [],
   "source": [
    "train_info = train_df.describe(include='all').transpose()\n",
    "test_info = test_df.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLOyqUDZqEYf"
   },
   "outputs": [],
   "source": [
    "train_info['DataType']=train_df.dtypes\n",
    "train_info['NullCount'] = train_df.isnull().sum()\n",
    "test_info['DataType']=test_df.dtypes\n",
    "test_info['NullCount'] = test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMKOzpQVZSr0"
   },
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3j1LDPCrOEv"
   },
   "outputs": [],
   "source": [
    "test_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GYhBhQXqbfL"
   },
   "outputs": [],
   "source": [
    "train_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3x6aDwQZSr0"
   },
   "outputs": [],
   "source": [
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7npo1narSDT"
   },
   "outputs": [],
   "source": [
    "test_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dznPTFX6ZSr0"
   },
   "source": [
    "#### **Result of Basic Exploration of Data**\n",
    "#### Based on the basic exploration above, you can now create a simple report of the data, noting down your observations regarding each column. Hence, creating a initial roadmap for further analysis.\n",
    "\n",
    "The selected columns in this are considered for further study and then a final list will be created\n",
    "\n",
    "1.   Gender -->  Nomianl Categorical Variable\n",
    "2.   Married --> Boolean Categorical Variable\n",
    "3.   Dependents --> Ordinal Categorical Variable\n",
    "4.   Education -->  Nominal Categorical Variable\n",
    "5.   Self_Employed --> Nominal Categorical Variable\n",
    "6.\t ApplicantIncome --> Continuous Variable \n",
    "7.\tCoapplicantIncome --> Continuous Variable\n",
    "8.\tLoanAmount --> Continuous Variable\n",
    "9.\tLoan_Amount_Term --> Continuous Variable \n",
    "10. Credit_History --> Categorical Boolean Variable \n",
    "11. Property_Area --> Categorical Nominal Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTb5sb9DZSr1"
   },
   "source": [
    "## 1.2 Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jq4t1XHQZSr1"
   },
   "source": [
    "##### Categorical variables: Bar plot\n",
    "##### Continuous variables: Histogram, Density & Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0QkX0QCvplS"
   },
   "outputs": [],
   "source": [
    "CategoricalColsList=[ 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area','Loan_Amount_Term','Loan_Status']\n",
    "ContinuousColsList = ['ApplicantIncome','CoapplicantIncome','LoanAmount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j8HKrhgZSr1"
   },
   "source": [
    "### 1.2.1 For Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBwyzGinZSr2"
   },
   "source": [
    "We can spot a categorical variable in the data by looking at the unique values in them. Typically a categorical variable contains less than 20 Unique values AND there is repetition of values, which means the data can be grouped by those unique values.\n",
    "\n",
    "Based on the Basic Data Exploration above, we have spotted 7 categorical predictors and 1 target variable in the data\n",
    "\n",
    "   ##### **Categorical Predictors:** 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area',Loan_Amount_Term\n",
    "   ##### **Target Variable** 'Loan_Status'\n",
    "\n",
    "We use bar charts to see how the data is distributed for these categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rd0RQ1bEZSr2"
   },
   "outputs": [],
   "source": [
    "def PlotBarCharts(inpData, colsToPlot):\n",
    "    %matplotlib inline   \n",
    "    import matplotlib.pyplot as plt   \n",
    "    # Generating multiple subplots\n",
    "    fig, subPlot=plt.subplots(nrows=1, ncols=len(colsToPlot), figsize=(40,6))\n",
    "    fig.suptitle('Bar charts of: '+ str(colsToPlot))\n",
    "    for colName, plotNumber in zip(colsToPlot, range(len(colsToPlot))):\n",
    "        inpData.groupby(colName).size().plot(kind='bar',ax=subPlot[plotNumber])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbFyWQbiZSr2"
   },
   "outputs": [],
   "source": [
    "PlotBarCharts(inpData=train_df, colsToPlot=CategoricalColsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1eFS_FUy_xW"
   },
   "outputs": [],
   "source": [
    "PlotBarCharts(inpData=test_df, colsToPlot=[ 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area','Loan_Amount_Term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4orhtGhru8n"
   },
   "outputs": [],
   "source": [
    "def label_function(val):\n",
    "    return f'{val / 100 * len(train_df):.0f}\\n{val:.0f}%'\n",
    "N = 50\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize=(20, 10))\n",
    "train_df.groupby('Gender').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax1)\n",
    "train_df.groupby('Married').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax2)\n",
    "train_df.groupby('Dependents').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green', 'violet', 'orange'], ax=ax3)\n",
    "train_df.groupby('Education').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax4)\n",
    "ax1.set_ylabel('Gender', size=15)\n",
    "ax2.set_ylabel('Married', size=15)\n",
    "ax3.set_ylabel('Dependents', size=15)\n",
    "ax4.set_ylabel('Education', size=15)\n",
    "plt.tight_layout()\n",
    "fig, (ax1, ax2, ax3,ax4) = plt.subplots(ncols=4, figsize=(20, 10))\n",
    "train_df.groupby('Self_Employed').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax1)\n",
    "train_df.groupby('Credit_History').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax2)\n",
    "train_df.groupby('Property_Area').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green', 'yellow'], ax=ax3)\n",
    "train_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax4)\n",
    "ax1.set_ylabel('Self_Employed', size=15)\n",
    "ax2.set_ylabel('Credit_History', size=15)\n",
    "ax3.set_ylabel('Property_Area', size=15)\n",
    "ax4.set_ylabel('Loan_Status', size=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXKjVQ0S5UXm"
   },
   "outputs": [],
   "source": [
    "N = 50\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize=(25, 10))\n",
    "test_df.groupby('Gender').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax1)\n",
    "test_df.groupby('Married').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax2)\n",
    "test_df.groupby('Dependents').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green', 'violet', 'orange'], ax=ax3)\n",
    "test_df.groupby('Education').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax4)\n",
    "ax1.set_ylabel('Gender', size=15)\n",
    "ax2.set_ylabel('Married', size=15)\n",
    "ax3.set_ylabel('Dependents', size=15)\n",
    "ax4.set_ylabel('Education', size=15)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(20, 8))\n",
    "test_df.groupby('Self_Employed').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax1)\n",
    "test_df.groupby('Credit_History').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax2)\n",
    "test_df.groupby('Property_Area').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green', 'yellow'], ax=ax3)\n",
    "ax1.set_ylabel('Self_Employed', size=15)\n",
    "ax2.set_ylabel('Credit_History', size=15)\n",
    "ax3.set_ylabel('Property_Area', size=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUSfG_tLZSr3"
   },
   "source": [
    "### 1.2.2 For Continuous Variables\n",
    "\n",
    "**Continuous Variable Predictors** - 'ApplicantIncome','CoapplicantIncome','LoanAmount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BTFfD-AoODN"
   },
   "outputs": [],
   "source": [
    "def PlotContinousCharts(inpData, colsToPlot):\n",
    "    %matplotlib inline   \n",
    "    plt.figure(figsize=(30, 50))\n",
    "    for i, col in enumerate(colsToPlot):\n",
    "      plt.subplot(9, 3, i*3+1)\n",
    "      plt.subplots_adjust(hspace =.25, wspace=.1)   \n",
    "      plt.grid(True)\n",
    "      plt.title('HistPlot : '+col)\n",
    "      sns.histplot(inpData[col], label=col, color = \"blue\")\n",
    "      plt.subplot(9, 3, i*3+2) \n",
    "      plt.title('BoxPlot : '+col)\n",
    "      sns.boxplot(inpData[col])\n",
    "      plt.subplot(9, 3, i*3+3) \n",
    "      plt.title('KDEPlot : '+col)\n",
    "      sns.kdeplot(inpData[col],shade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuprKtZvobSC"
   },
   "outputs": [],
   "source": [
    "PlotContinousCharts(inpData=train_df, colsToPlot=ContinuousColsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLmDSm3f0ny6"
   },
   "outputs": [],
   "source": [
    "PlotContinousCharts(inpData=test_df, colsToPlot=ContinuousColsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Yfie6JlZSr5"
   },
   "source": [
    "## 1.3 Bivariate Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh6uw_XyNa29"
   },
   "source": [
    "### 1.3.1 For Categorical Variables with Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuJtnQh4ZSr6"
   },
   "outputs": [],
   "source": [
    "cols = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area','Loan_Amount_Term']\n",
    "nr_rows = 2\n",
    "nr_cols = 4\n",
    "fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4.5,nr_rows*4))\n",
    "for r in range(0,nr_rows):\n",
    "    for c in range(0,nr_cols):          \n",
    "        i = r*nr_cols+c       \n",
    "        ax = axs[r][c]\n",
    "        sns.countplot(train_df[cols[i]], hue=train_df[\"Loan_Status\"], ax=ax,palette=['#347C17',\"#FF2400\"])\n",
    "        ax.set_title(cols[i], fontsize=12, fontweight='bold')\n",
    "        ax.legend(title=\"Loan Status\", loc='best')    \n",
    "        for p in ax.patches:\n",
    "            ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))        \n",
    "plt.tight_layout()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2GIIfGjPs_E"
   },
   "source": [
    "#### Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWKy2bRR891d"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2,ax3,ax4) = plt.subplots(ncols=4, figsize=(40, 15))\n",
    "train_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['blue', 'orange'],ax=ax1)\n",
    "train_df.groupby('Gender').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], radius=0.7,startangle=90,ax=ax1)\n",
    "ax1.set_ylabel('Gender', size=15)\n",
    "train_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['blue', 'orange'],ax=ax2)\n",
    "train_df.groupby('Married').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], radius=0.7,startangle=90,ax=ax2)\n",
    "ax2.set_ylabel('Married', size=15)\n",
    "train_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['blue', 'orange'],ax=ax3)\n",
    "train_df.groupby('Dependents').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green', 'violet', 'yellow'], radius=0.7,startangle=90,ax=ax3)\n",
    "ax3.set_ylabel('Dependents', size=15)\n",
    "train_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['blue', 'orange'],ax=ax4)\n",
    "train_df.groupby('Education').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], radius=0.7,startangle=90,ax=ax4)\n",
    "ax4.set_ylabel('Education', size=15)\n",
    "\n",
    "fig, (ax1, ax2,ax3) = plt.subplots(ncols=3, figsize=(20, 7))\n",
    "train_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['blue', 'orange'],ax=ax1)\n",
    "train_df.groupby('Self_Employed').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], radius=0.7,startangle=90,ax=ax1)\n",
    "ax1.set_ylabel('Self_Employed', size=15)\n",
    "train_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['blue', 'orange'],ax=ax2)\n",
    "train_df.groupby('Credit_History').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], radius=0.7,startangle=90,ax=ax2)\n",
    "ax2.set_ylabel('Credit_History', size=15)\n",
    "train_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['blue', 'orange'],ax=ax3)\n",
    "train_df.groupby('Property_Area').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green', 'violet', 'yellow'], radius=0.7,startangle=90,ax=ax3)\n",
    "ax3.set_ylabel('Property_Area', size=15)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7y3hgVgw--2S"
   },
   "outputs": [],
   "source": [
    "labels = ['vegetable', 'fruit']\n",
    "sizes = [300, 200]\n",
    "labels_vegefruit = ['potato', 'tomato', 'onion', 'apple',\n",
    "                    'banana', 'cherry', 'durian']\n",
    "sizes_vegefruit = [170, 70, 60, 70, 60, 50, 20]\n",
    "colors = ['#FFB600', '#09A0DA']\n",
    "colors_vegefruit = ['#FFCE53', '#FFDA7E', '#FFE9B2', '#30B7EA',\n",
    "                    '#56C7F2','#7FD6F7', '#B3E7FB']\n",
    " \n",
    "bigger = plt.pie(sizes, labels=labels, colors=colors,\n",
    "                 startangle=90, frame=True)\n",
    "smaller = plt.pie(sizes_vegefruit, labels=labels_vegefruit,\n",
    "                  colors=colors_vegefruit, radius=0.7,\n",
    "                  startangle=90, labeldistance=0.7)\n",
    "centre_circle = plt.Circle((0, 0), 0.4, color='white', linewidth=0)\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "        \n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPHiObFAZSr6"
   },
   "source": [
    "### 1.3.2 For Continuous Variable with Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7-gBddZ5FAx"
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1)\n",
    "continous_var = ContinuousColsList\n",
    "plt.figure(figsize=(30, 50))\n",
    "for i, col in enumerate(continous_var):\n",
    "    plt.subplot(9, 3, i*3+1)\n",
    "    plt.subplots_adjust(hspace =.25, wspace=.1)\n",
    "    plt.grid(True)\n",
    "    plt.title('Bivariate Analysis with Density Plot : '+col,fontsize=12, fontweight='bold')\n",
    "    ax=sns.kdeplot(train_df.loc[train_df[\"Loan_Status\"]=='N', col], label=\"N\", color = \"red\", fill=True)\n",
    "    ax=sns.kdeplot(train_df.loc[train_df[\"Loan_Status\"]=='Y', col], label=\"Y\",  color = \"green\", fill=True)\n",
    "    ax.legend(title=\"Loan Status\", loc='best') \n",
    "    plt.subplot(9, 3, i*3+2) \n",
    "    ax1=sns.boxplot(y = col, data = train_df, x=\"Loan_Status\", palette = [\"red\", \"green\"])\n",
    "    plt.title('Bivariate Analysis with Box Plot : '+col,fontsize=12, fontweight='bold')\n",
    "    plt.subplot(9, 3, i*3+3) \n",
    "    ax2=sns.histplot(train_df.loc[train_df[\"Loan_Status\"]=='N', col], label=\"N\", color = \"red\")\n",
    "    ax2=sns.histplot(train_df.loc[train_df[\"Loan_Status\"]=='Y', col], label=\"Y\",  color = \"green\")\n",
    "    ax2.legend(title=\"Loan Status\", loc='best') \n",
    "    plt.title('Bivariate Analysis with Histogram Plot : '+col,fontsize=12, fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EW6Ht8OHZSr7"
   },
   "source": [
    "## 1.4 Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQ-a7ToNN1sc"
   },
   "source": [
    "### 1.4.1 Pair Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDISWiMnZSr7"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(train_df,hue='Loan_Status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcA4QrazN-fp"
   },
   "source": [
    "### 1.4.2 Clustermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qS65fDkJNVI"
   },
   "outputs": [],
   "source": [
    "corr = train_df.corr()\n",
    "sns.set(font_scale=1)\n",
    "sns.clustermap(corr, cmap='BuGn', vmax=.3, center=0,square=False, linewidths=.5,annot=True, cbar_kws={\"shrink\": .5},annot_kws={\"size\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfazL65IMlJ-"
   },
   "source": [
    "# **2. Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipTrdIIYZSr8"
   },
   "source": [
    "## 2.1 Handling Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAGL6TRakM34"
   },
   "source": [
    "### 2.1.1 Plotting Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRIvx3Xb7q79"
   },
   "outputs": [],
   "source": [
    "train_imp = train_df\n",
    "train_imp.drop('Loan_ID',  axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oViidjh8ZSr8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.heatmap(train_imp.isna().transpose(),cmap=\"BuGn\",cbar_kws={'label': 'Missing Data'})\n",
    "plt.title(\" Missing Values in the given training data\")\n",
    "sns.set(font_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdemPTvaj5OY"
   },
   "outputs": [],
   "source": [
    "cat_null =[ 'Gender', 'Married', 'Dependents',  'Self_Employed', 'Credit_History', 'Loan_Amount_Term']\n",
    "con_null = ['ApplicantIncome','CoapplicantIncome','LoanAmount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jmDxde0XS_K"
   },
   "source": [
    "### 2.1.2 For Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ImOUKYPXRbK"
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Run the imputer with a simple Random Forest estimator\n",
    "imp = IterativeImputer(RandomForestRegressor(n_estimators=5), max_iter=5, random_state=1)\n",
    "to_train = con_null\n",
    "#perform filling\n",
    "train_imp[to_train] = pd.DataFrame(imp.fit_transform(train_imp[to_train]), columns=to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SsBM02veJ2Y"
   },
   "source": [
    "### 2.1.3 For Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWkgj6aOXZ_F"
   },
   "outputs": [],
   "source": [
    "# Imputer object using the mean strategy and \n",
    "# missing_values type for imputation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "train_imp[cat_null] = train_imp[cat_null].apply(lambda series: pd.Series(LabelEncoder().fit_transform(series[series.notnull()]),index=series[series.notnull()].index))\n",
    "imp_cat = IterativeImputer(estimator=RandomForestClassifier(),initial_strategy='most_frequent',max_iter=10, random_state=0)\n",
    "train_imp[cat_null] = imp_cat.fit_transform(train_imp[cat_null])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-khLY9p8hXzp"
   },
   "source": [
    "### 2.1.4 Result : Heat Map after Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iqx38jLzZSr9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.heatmap(train_imp.isna().transpose(),cmap=\"YlGnBu\",cbar_kws={'label': 'Missing Data'})\n",
    "plt.title(\"After Handling Missing Values in the given training data\")\n",
    "sns.set(font_scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ctJ8zVjZSr9"
   },
   "source": [
    "## 2.2 Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSD_TqpmNVig"
   },
   "source": [
    "#### 2.2.1 For Continuous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cm1-yYvZSr-"
   },
   "source": [
    "Outliers are extreme values in the data which are far away from most of the values. You can see them as the tails in the histogram.\n",
    "\n",
    "Outlier must be treated one column at a time. As the treatment will be slightly different for each column.Outliers bias the training of machine learning models. As the algorithm tries to fit the extreme value, it goes away from majority of the data.\n",
    "\n",
    "There are below two options to treat outliers in the data.\n",
    "\n",
    "    Option-1: Delete the outlier Records. Only if there are just few rows lost.\n",
    "    Option-2: Impute the outlier values with a logical business value                                         \n",
    "Below we are finding out the **Inter Quartile Range Method** outliers by looking at the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjaipmO4kWpW"
   },
   "outputs": [],
   "source": [
    "for x in ['ApplicantIncome','CoapplicantIncome','LoanAmount']:\n",
    "    q75,q25 = np.percentile(train_imp.loc[:,x],[75,25])\n",
    "    intr_qr = q75-q25\n",
    "    max = q75+(1.5*intr_qr)\n",
    "    min = q25-(1.5*intr_qr)\n",
    "    train_imp.loc[train_imp[x] < min,x] = min\n",
    "    train_imp.loc[train_imp[x] > max,x] = max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1OIOZ4bZSsA"
   },
   "source": [
    "## 2.3 Categorical variable with Target Value after handling outliers and missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbjYttz3ZSsA"
   },
   "outputs": [],
   "source": [
    "cols = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area','Loan_Amount_Term']\n",
    "nr_rows = 2\n",
    "nr_cols = 4\n",
    "fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4.5,nr_rows*4))\n",
    "for r in range(0,nr_rows):\n",
    "    for c in range(0,nr_cols):          \n",
    "        i = r*nr_cols+c       \n",
    "        ax = axs[r][c]\n",
    "        sns.countplot(train_imp[cols[i]], hue=train_imp[\"Loan_Status\"], ax=ax,palette=['#347C17',\"#FF2400\"])\n",
    "        ax.set_title(cols[i], fontsize=12, fontweight='bold')\n",
    "        ax.legend(title=\"Loan Status\", loc='best')    \n",
    "        for p in ax.patches:\n",
    "            ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))        \n",
    "plt.tight_layout()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4C3CDIp7ZSr_"
   },
   "source": [
    "## 2.4 Continuous variable with Target Value after handling outliers and missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0Qipn-jBKWu"
   },
   "outputs": [],
   "source": [
    "a = 4  # number of rows\n",
    "b = 3  # number of columns\n",
    "c = 1  # initialize plot counter\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "for i in ContinuousColsList:\n",
    "    plt.subplot(a, b, c)\n",
    "    plt.xlabel(i)\n",
    "    sns.histplot(x=train_imp[i],hue=train_imp['Loan_Status'],palette = [\"green\", \"red\"])\n",
    "    c = c + 1\n",
    "    plt.subplot(a, b, c)\n",
    "    plt.xlabel(i)\n",
    "    sns.boxplot(y = i, data = train_imp, x='Loan_Status', palette = [\"green\", \"red\"])\n",
    "    c = c + 1\n",
    "    plt.subplot(a, b, c)\n",
    "    plt.xlabel(i)\n",
    "    sns.kdeplot(data = train_imp, x = i, hue = 'Loan_Status',fill=True,palette = [\"green\", \"red\"])\n",
    "    c = c + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlsIn9ISn7aL"
   },
   "source": [
    "## 2.5 Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MDvxWVt8SXj"
   },
   "source": [
    "### 2.5.1 Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQY_DZsqGt8A"
   },
   "source": [
    "From the above plots, it is proved that we have removed outliers by using IQR method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kGMXVoUoGQi"
   },
   "source": [
    "List of steps performed on predictor variables before data can be used for machine learning\n",
    "1.   Converting each Ordinal Categorical columns to numeric\n",
    "2.   Converting Binary nominal Categorical columns to numeric using 1/0 mapping\n",
    "3.  Converting all other nominal categorical columns to numeric using pd.get_dummies()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTkv2DBIn0SA"
   },
   "outputs": [],
   "source": [
    "train_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJwomNKgn0VX"
   },
   "outputs": [],
   "source": [
    "train_enc = train_imp\n",
    "# Treating the binary nominal variables first\n",
    "train_enc['Loan_Status'].replace({'Y':1, 'N':0}, inplace=True)\n",
    "# Treating all the nominal variables at once using dummy variables\n",
    "train_enc=pd.get_dummies(train_enc)\n",
    "train_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7AaX2rrZSsA"
   },
   "source": [
    "## 2.6 Feature Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXrhMXFAO8hU"
   },
   "source": [
    "#### Defining Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfHVRFlZp7ib"
   },
   "outputs": [],
   "source": [
    "train_enc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGNqKIgXqEdj"
   },
   "outputs": [],
   "source": [
    "inp_cat = ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History', 'Education_Graduate', 'Education_Not Graduate',\n",
    "       'Property_Area_Rural', 'Property_Area_Semiurban','Property_Area_Urban']\n",
    "inp_con = ['ApplicantIncome','CoapplicantIncome','LoanAmount']\n",
    "out_cat = ['Loan_Status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QI-G8PIXwRFT"
   },
   "source": [
    "\n",
    "Now its time to finally choose the best columns(Features) which are correlated to the Target variable. This can be done directly by measuring the correlation values or ANOVA/Chi-Square tests.\n",
    "\n",
    "However, we have visualized the relation between the Target variable and each of the predictors to get a better sense of data in Bivariate Analysis.\n",
    "\n",
    "In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons:\n",
    "\n",
    "\n",
    "\n",
    "> simplification of models to make them easier to interpret by researchers/users\n",
    "\n",
    "\n",
    "> shorter training times\n",
    "\n",
    "\n",
    "> enhanced generalization by reducing overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smfnk0NQdVCH"
   },
   "outputs": [],
   "source": [
    "train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmhoDjmYaIVv"
   },
   "outputs": [],
   "source": [
    "train_features = train_enc\n",
    "y = train_features['Loan_Status']\n",
    "X = train_features.drop('Loan_Status',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFwj1KlEUSwV"
   },
   "source": [
    "### Univariate Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wCR2qYrwxvB"
   },
   "source": [
    "**chi2, f_classif, mutual_info_classif**\n",
    "The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Cl5JeXBi_IO"
   },
   "source": [
    "#### **KBest with f_classif** \n",
    "\n",
    "We can select features according to the k highest scores. Along with ANOVA F-value Score between label/feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIARQ191uoZ9"
   },
   "outputs": [],
   "source": [
    "# ANOVA feature selection for numeric input and categorical output\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "bestfeatures = SelectKBest(score_func=f_classif, k=14)\n",
    "fit = bestfeatures.fit(X, y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "df_fclassif = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "df_fclassif.columns = ['Features','fclass_Score']  ## naming the dataframe columns\n",
    "#print(featureScores.nlargest(10,'Score'))  ## print 10 best features\n",
    "plt.barh(df_fclassif['Features'],df_fclassif['fclass_Score'],color='green')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Feature Score')\n",
    "plt.title('Feature Selection using KBest,F_Classif',fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tOkCPOVlAd8"
   },
   "source": [
    "#### **KBest with chi2** \n",
    "\n",
    "We can select features according to the k highest scores. Chi-squared stats of non-negative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUS2g3KqMqnz"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=14)\n",
    "fit = bestfeatures.fit(X, y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "df_chi = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "df_chi.columns = ['Features','chi_Score']  ## naming the dataframe columns\n",
    "#print(featureScores.nlargest(10,'Score'))  ## print 10 best features\n",
    "plt.barh(df_chi['Features'],df_chi['chi_Score'],color='violet')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Feature Score')\n",
    "plt.title('Feature Selection using KBest,Chi_Score',fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7JeK0JQWrWK"
   },
   "outputs": [],
   "source": [
    "df_KBest=df_chi\n",
    "df_KBest['F_classif_Score'] = df_fclassif['fclass_Score']\n",
    "df_KBest.plot(x='Features',y=['chi_Score','F_classif_Score'],kind='barh')\n",
    "plt.xlabel('Feature Score')\n",
    "plt.title('Feature Selection using Chi_Score,F_classif',fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d43UVX9lz3m"
   },
   "source": [
    "#### **KBest with Mutual Information** \n",
    "\n",
    "We can select features according to the k highest scores. Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3zD29czFcSp"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "selector = SelectKBest(mutual_info_classif, k=14)\n",
    "selector.fit_transform(X, y)\n",
    "dfscores = pd.DataFrame(selector.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "df_mc_score = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "df_mc_score.columns = ['Features','mc_Score']  ## naming the dataframe columns\n",
    "#plt.barh(X.columns, model.feature_importances_,color='blue')\n",
    "plt.barh(df_mc_score['Features'],df_mc_score['mc_Score'],color='orange')\n",
    "plt.xlabel('Feature Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Selection using KBest,Mutual_Info_Classifier',fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igELlXXmwzeX"
   },
   "source": [
    "### Feature Importance using Extra Tree Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGSXn3wuOjJV"
   },
   "source": [
    "Extremely Randomized Trees Classifier(Extra Trees Classifier) is a type of ensemble learning technique which aggregates the results of multiple de-correlated decision trees collected in a “forest” to output it’s classification result. In concept, it is very similar to a Random Forest Classifier and only differs from it in the manner of construction of the decision trees in the forest.\n",
    "\n",
    "Each Decision Tree in the Extra Trees Forest is constructed from the original training sample. Then, at each test node, Each tree is provided with a random sample of k features from the feature-set from which each decision tree must select the best feature to split the data based on some mathematical criteria (typically the Gini Index). This random sample of features leads to the creation of multiple de-correlated decision trees.\n",
    "\n",
    "To perform feature selection using the above forest structure, during the construction of the forest, for each feature, the normalized total reduction in the mathematical criteria used in the decision of feature of split (Gini Index if the Gini Index is used in the construction of the forest) is computed. This value is called the Gini Importance of the feature. To perform feature selection, each feature is ordered in descending order according to the Gini Importance of each feature and the user selects the top k features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iIwguECRpfr"
   },
   "outputs": [],
   "source": [
    "# Feature Importance with Extra Trees Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# load data\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier(n_estimators=14)\n",
    "model.fit(X, y)\n",
    "# Normalizing the individual importances\n",
    "#feature_importance_normalized = np.std([model.feature_importances_ for tree in model.estimators_],axis = 0)\n",
    "# Plotting a Bar Graph to compare the models\n",
    "dfscores = pd.DataFrame(model.feature_importances_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "df_extra_score = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "df_extra_score.columns = ['Features','etree_imp']  ## naming the dataframe columns\n",
    "#plt.barh(X.columns, model.feature_importances_,color='blue')\n",
    "plt.barh(df_extra_score['Features'],df_extra_score['etree_imp'],color='blue')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importances using Extra Tree Classifier',fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCItRtJtq54m"
   },
   "source": [
    "### Feature Ranking using Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHiJr9pKOpiR"
   },
   "source": [
    "\n",
    "Recursive Feature Elimination, or RFE for short, is a popular feature selection algorithm.\n",
    "\n",
    "RFE is popular because it is easy to configure and use and because it is effective at selecting those features (columns) in a training dataset that are more or most relevant in predicting the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3GSNt5gzrqo"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, y)\n",
    "# Plotting a Bar Graph to compare the models\n",
    "dfscores = pd.DataFrame(fit.ranking_)\n",
    "dfsupport = pd.DataFrame(fit.support_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "df_log_score = pd.concat([dfcolumns,dfscores,dfsupport],axis=1)\n",
    "#print(df_log_score.head())\n",
    "df_log_score.columns = ['Features','rfe_rank','Support']  ## naming the dataframe columns\n",
    "plt.barh(df_log_score['Features'],df_log_score['rfe_rank'],color='teal')\n",
    "plt.xlabel('Feature Ranking')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Ranking using Recursive Feature Elimination',fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "plt.barh(df_log_score['Features'],df_log_score['Support'],color='brown')\n",
    "plt.xlabel('Feature Support')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Support using Recursive Feature Elimination',fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_luNc47QwCRO"
   },
   "source": [
    "### Feature Selection using Correlation Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OavS0N66OurY"
   },
   "source": [
    "The correlation feature selection (CFS) measure evaluates subsets of features on the basis of the following hypothesis: \"Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMUv0p8xRpuN"
   },
   "outputs": [],
   "source": [
    "train_corr = train_enc.corr()['Loan_Status'].sort_values(ascending=True).head(14)\n",
    "top_corr_features = train_corr.index\n",
    "train_corr.plot(kind='barh')\n",
    "plt.xlabel('Corelation Index')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Selection using Correlation Method',fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4a_kFDlBx2pI"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "matrix = np.triu(train_enc.corr())\n",
    "sns.heatmap(train_enc.corr(), annot=True, mask=matrix,cbar_kws= {'orientation': 'horizontal'} , vmin=-1, vmax=1, center= 0, cmap=\"YlGnBu\")\n",
    "#sns.heatmap(train_imp.isna().transpose(),cmap=\"YlGnBu\",cbar_kws={'label': 'Missing Data'})\n",
    "plt.title(\"Correlation of Features\")\n",
    "sns.set(font_scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhExplyVrqv3"
   },
   "source": [
    "# **3. Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WFjEwk7Lp-G"
   },
   "source": [
    "## 3.1 Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktfgdmiJQYo_"
   },
   "source": [
    "### 3.1.1 Bar Chart for Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUSZ64Mir5BT"
   },
   "outputs": [],
   "source": [
    "PlotBarCharts(inpData=train_enc, colsToPlot=['Gender', 'Married', 'Dependents', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History', 'Education_Graduate', 'Education_Not Graduate',\n",
    "       'Property_Area_Rural', 'Property_Area_Semiurban','Property_Area_Urban','Loan_Status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5x4lso8lL22u"
   },
   "source": [
    "### 3.1.2 Plots for Continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjrOG3kBr5PG"
   },
   "outputs": [],
   "source": [
    "PlotContinousCharts(inpData=train_enc, colsToPlot=ContinuousColsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGscM8QAMEsT"
   },
   "source": [
    "## 3.2 Bivariate Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tN9xAFrzQSq8"
   },
   "source": [
    "### 3.2.1 For Categorical Variables with Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0gBH9kwbl6o"
   },
   "outputs": [],
   "source": [
    "cols = ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History', 'Education_Graduate', 'Education_Not Graduate',\n",
    "       'Property_Area_Rural', 'Property_Area_Semiurban','Property_Area_Urban','Loan_Status']\n",
    "nr_rows = 3\n",
    "nr_cols = 4\n",
    "fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4.5,nr_rows*4))\n",
    "for r in range(0,nr_rows):\n",
    "    for c in range(0,nr_cols):          \n",
    "        i = r*nr_cols+c       \n",
    "        ax = axs[r][c]\n",
    "        sns.countplot(train_enc[cols[i]], hue=train_enc[\"Loan_Status\"], ax=ax,palette=['#FF2400',\"#347C17\"])\n",
    "        ax.set_title(cols[i], fontsize=12, fontweight='bold')\n",
    "        ax.legend(title=\"Loan Status\", loc='best')    \n",
    "        for p in ax.patches:\n",
    "            ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))        \n",
    "plt.tight_layout()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZQCq_51NW5U"
   },
   "source": [
    "### 3.2.2 For Continuous Variable with Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKF-tlNVbmCX"
   },
   "outputs": [],
   "source": [
    "a = 4  # number of rows\n",
    "b = 3  # number of columns\n",
    "c = 1  # initialize plot counter\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "for i in ContinuousColsList:\n",
    "    plt.subplot(a, b, c)\n",
    "    plt.xlabel(i)\n",
    "    sns.histplot(x=train_enc[i],hue=train_enc['Loan_Status'],palette = [\"red\", \"green\"])\n",
    "    c = c + 1\n",
    "    plt.subplot(a, b, c)\n",
    "    plt.xlabel(i)\n",
    "    sns.boxplot(y = i, data = train_enc, x='Loan_Status', palette = [\"red\", \"green\"])\n",
    "    c = c + 1\n",
    "    plt.subplot(a, b, c)\n",
    "    plt.xlabel(i)\n",
    "    sns.kdeplot(data = train_enc, x = i, hue = 'Loan_Status',fill=True,palette = [\"red\", \"green\"])\n",
    "    c = c + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CugW_D0LNjqB"
   },
   "source": [
    "## 3.3 Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eAMx_KfQr3g"
   },
   "source": [
    "### 3.3.1 Pair Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uof_VmaFbmIu"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(train_enc,hue='Loan_Status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJqBBhEwNueh"
   },
   "source": [
    "### 3.3.2 Correlation Plot with Clustermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITp9xBBHecr2"
   },
   "outputs": [],
   "source": [
    "corr = train_enc.corr()\n",
    "sns.set(font_scale=1)\n",
    "sns.clustermap(corr, cmap='Greens', vmax=.8, center=0,square=False, linewidths=.5,annot=True, cbar_kws={\"shrink\": .5},annot_kws={\"size\": 10},figsize=(17, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlMtU94NZSsD"
   },
   "source": [
    "# **4. Creation & Evaluation of classifcation models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWQ8jRJvMMEp"
   },
   "source": [
    "#### Importing Classification Model Libraries and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnfPrglUZSsG"
   },
   "outputs": [],
   "source": [
    "#sklearn\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold,learning_curve,ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve,f1_score,log_loss,brier_score_loss,fbeta_score\n",
    "from sklearn import svm,model_selection, tree, linear_model, naive_bayes, ensemble,gaussian_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdAStZSCZSsE"
   },
   "source": [
    "### 3.1. Preparing Data for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIoFINJlMDCg"
   },
   "source": [
    "#### Definining Predictors and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Yt_kezWZSsE"
   },
   "outputs": [],
   "source": [
    "print(train_enc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEZgYNoVZSsE"
   },
   "outputs": [],
   "source": [
    "# Separate Target Variable and Predictor Variables\n",
    "TargetVariable='Loan_Status'\n",
    "Predictors_All=['Gender', 'Married', 'Dependents', 'Self_Employed', 'ApplicantIncome','CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History','Education_Graduate', 'Education_Not Graduate',\n",
    "       'Property_Area_Rural', 'Property_Area_Semiurban','Property_Area_Urban']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcW2Nv_Y5XPb"
   },
   "source": [
    "#### Define Classification Algorithms for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8iV5l7qZSsH"
   },
   "outputs": [],
   "source": [
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(n_estimators=75, random_state=7),\n",
    "    ensemble.BaggingClassifier(n_estimators=75, random_state=7),\n",
    "    ensemble.ExtraTreesClassifier(n_estimators=75, random_state=7),\n",
    "    ensemble.GradientBoostingClassifier(n_estimators=75, random_state=7),\n",
    "    ensemble.RandomForestClassifier(n_estimators=75, random_state=7),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    naive_bayes.MultinomialNB(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),   \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mC-QTTeh5k7m"
   },
   "source": [
    "#### Define Dataframe for Different Performance Values for Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrGgicoPZSsH"
   },
   "outputs": [],
   "source": [
    "def Models(Xtrain,ytrain,xtest,ytest):\n",
    "  MLA_columns = []\n",
    "  MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "  kfold = model_selection.KFold(n_splits=10, random_state=None)\n",
    "  rfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "  row_index = 0\n",
    "  for alg in MLA:    \n",
    "      predicted = alg.fit(X_train, y_train).predict(X_test)\n",
    "      fp, tp, th = roc_curve(y_test, predicted)\n",
    "      MLA_name = alg.__class__.__name__\n",
    "      MLA_compare.loc[row_index,'Model Name'] = MLA_name\n",
    "      MLA_compare.loc[row_index, 'Train Accuracy'] = round(alg.score(X_train, y_train), 4)\n",
    "      MLA_compare.loc[row_index, 'Test Accuracy'] = round(alg.score(X_test, y_test), 4)\n",
    "      MLA_compare.loc[row_index, 'Precision'] = round(precision_score(y_test, predicted),4)\n",
    "      MLA_compare.loc[row_index, 'Recall'] = round(recall_score(y_test, predicted),4)\n",
    "      MLA_compare.loc[row_index, 'AUC'] = auc(fp, tp)\n",
    "      MLA_compare.loc[row_index, 'f1_macro'] = round(f1_score(y_test, predicted, average = \"macro\"),4)\n",
    "      MLA_compare.loc[row_index, 'f1_micro'] = round(f1_score(y_test, predicted, average = \"micro\"),4)\n",
    "      MLA_compare.loc[row_index, 'f1_weighted'] = round(f1_score(y_test, predicted, average = \"weighted\"),4)\n",
    "      MLA_compare.loc[row_index, 'f_beta'] = round(fbeta_score(y_test, predicted, beta=4),4)\n",
    "      MLA_compare.loc[row_index, 'Logloss'] = round(log_loss(y_test, predicted),4)\n",
    "      MLA_compare.loc[row_index, 'Brier Score Loss'] = round(brier_score_loss(y_test, predicted),4)\n",
    "      row_index+=1\n",
    "  return MLA_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHe6c6Uj5-c_"
   },
   "source": [
    "#### Define ROC Curve Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9NkjS8SZSsI"
   },
   "outputs": [],
   "source": [
    "def PlotRocCurve(X_train,y_train,X_test,y_test):\n",
    "  index = 0\n",
    "  fig, ax = plt.subplots(1, figsize=(15, 8))\n",
    "  for alg in MLA:\n",
    "      predicted = alg.fit(X_train, y_train).predict(X_test)\n",
    "      fp, tp, th = roc_curve(y_test, predicted)\n",
    "      roc_auc_mla = auc(fp, tp)\n",
    "      MLA_name = alg.__class__.__name__\n",
    "      plt.plot(fp, tp, lw=2, alpha=0.3, label='ROC %s (AUC = %0.2f)'  % (MLA_name, roc_auc_mla)) \n",
    "      index=index+1\n",
    "\n",
    "  plt.title('ROC Curve comparison',fontsize=14, fontweight='bold')\n",
    "  plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "  plt.plot([0,1],[0,1],'b--')\n",
    "  plt.xlim([0,1])\n",
    "  plt.grid(False)\n",
    "  plt.ylim([0,1])\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.xlabel('False Positive Rate')    \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bB0DuzEf6HkF"
   },
   "source": [
    "#### Define Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhE0sfb5ZSsI"
   },
   "outputs": [],
   "source": [
    "def PlotConfusionMatrix(X_test, y_test):\n",
    "  import sklearn\n",
    "  index = 0\n",
    "  for alg in MLA:\n",
    "      MLA_name = alg.__class__.__name__\n",
    "      sklearn.metrics.plot_confusion_matrix(alg, X_test, y_test, display_labels=['N', 'Y'],cmap=plt.cm.Blues)\n",
    "      plt.grid(False)\n",
    "      plt.title('Confusion Matrix : '+MLA_name,fontsize=14, fontweight='bold')\n",
    "      index+=1  \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHL3cDc_6RXr"
   },
   "source": [
    "#### Define Plotting of Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKQh4dUJp755"
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(25, 7))\n",
    "\n",
    "    axes[0].set_title('Learning Curve of the Model : '+title,fontsize=14, fontweight='bold')\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"darkgreen\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, '*-', color=\"darkred\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-',color='orange')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1,color='darkorange')\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the Model : \"+title,fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-',color='darkblue')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,color='b')\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the Model : \"+title,fontsize=14, fontweight='bold')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq8CCsKZKRPd"
   },
   "source": [
    "#### Plotting Performance Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XD3duf6dqkmC"
   },
   "outputs": [],
   "source": [
    "def plotperfcurves(X,y):\n",
    "  index = 0\n",
    "  for alg in MLA:\n",
    "      MLA_name = alg.__class__.__name__\n",
    "      estimator = alg\n",
    "      title = MLA_name\n",
    "      plot_learning_curve(estimator, title, X, y,  ylim=(0.4, 1.05),cv=5, n_jobs=4)\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv8yfw7DgJyz"
   },
   "source": [
    "### BinaryClass ROC Curve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsJuAvnZZSsE"
   },
   "source": [
    "### 3.2 Splitting to training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ir7t6O_jl3yb"
   },
   "outputs": [],
   "source": [
    "X = train_enc.drop(columns=['Loan_Status'])\n",
    "y = train_enc['Loan_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZYjfQsyZSsE"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LK3bGkZtKZUZ"
   },
   "source": [
    "### **Output 1** - All Features considered in model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yse63xAiGNPI"
   },
   "source": [
    "#### Learning Curve, Scalability and Performance Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewLzGR05w0Zs"
   },
   "outputs": [],
   "source": [
    "plotperfcurves(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNfWKBD1I4vD"
   },
   "source": [
    "#### Classification Model Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlGubxpFUYyM"
   },
   "outputs": [],
   "source": [
    "All_Features = Models(X_train,y_train,X_test,y_test)\n",
    "All_Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cvlfFo6IzXc"
   },
   "source": [
    "#### F Score Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8WXujurocm4"
   },
   "outputs": [],
   "source": [
    "All_Features.plot(x=\"Model Name\",y=[\"f1_macro\",\"f1_micro\",\"f1_weighted\",\"f_beta\"],kind=\"bar\",figsize=(30, 7))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Model Comparision with different f-scores',fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNEwwiPhIvon"
   },
   "source": [
    "#### Log Loss Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCHykUx5rw70"
   },
   "outputs": [],
   "source": [
    "All_Features.plot(x=\"Model Name\",y=[\"Logloss\"],kind=\"barh\",figsize=(10, 5))\n",
    "plt.xticks(rotation=360)\n",
    "plt.title('Log Loss for Different Models',fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojHbsZjhIqum"
   },
   "source": [
    "#### ROC Comparision Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTFHT8FIY_OW"
   },
   "outputs": [],
   "source": [
    "PlotRocCurve(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTr08m5LNMG7"
   },
   "source": [
    "### **Output 2** - With Reduced Features considered in model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxt_GH4dZSsF"
   },
   "outputs": [],
   "source": [
    "train= X_train\n",
    "train['Loan_Status']=y_train\n",
    "test=X_test\n",
    "test['Loan_Status']=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZDIWVfxNihl"
   },
   "outputs": [],
   "source": [
    "X_train_new = train[['Gender', 'Married', 'Credit_History','Education_Graduate','Property_Area_Rural', 'Property_Area_Semiurban','Property_Area_Urban']]\n",
    "y_train_new= y_train\n",
    "X_test_new = X_test[['Gender', 'Married', 'Credit_History','Education_Graduate', 'Property_Area_Rural', 'Property_Area_Semiurban','Property_Area_Urban']]\n",
    "y_test_new =y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ex-pFG1mOYa_"
   },
   "outputs": [],
   "source": [
    "y_red = train_features['Loan_Status']\n",
    "X_red = train_features[['Gender', 'Married', 'Credit_History','Education_Graduate','Property_Area_Rural', 'Property_Area_Semiurban','Property_Area_Urban']]\n",
    "X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OBoTy2EIcVO"
   },
   "source": [
    "#### Classification Model Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iycpq4rNIVo4"
   },
   "outputs": [],
   "source": [
    "Reduced_Features = Models(X_train_new,y_train_new,X_test_new,y_test_new)\n",
    "Reduced_Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKrTGvU4IGOD"
   },
   "source": [
    "#### ROC Comparision Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7hHTQB_afLJ"
   },
   "outputs": [],
   "source": [
    "PlotRocCurve(X_train_red,y_train_red,X_test_red,y_test_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_zZNNUxH96G"
   },
   "source": [
    "#### F Score Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcZ0P9qloUkk"
   },
   "outputs": [],
   "source": [
    "Reduced_Features.plot(x=\"Model Name\",y=[\"f1_macro\",\"f1_micro\",\"f1_weighted\",\"f_beta\"],kind=\"bar\",figsize=(30, 7))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Model Comparision with different f-scores',fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMF6VB-fH4z0"
   },
   "source": [
    "#### Log Loss Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pP6kBul0rp_d"
   },
   "outputs": [],
   "source": [
    "Reduced_Features.plot(x=\"Model Name\",y=[\"Logloss\"],kind=\"barh\",figsize=(10, 5))\n",
    "plt.xticks(rotation=360)\n",
    "plt.title('Log Loss for Different Models',fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uK_g2vQWGiWb"
   },
   "source": [
    "#### Learning Curve, Scalability and Performance Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxHgyHN_OCUj"
   },
   "outputs": [],
   "source": [
    "plotperfcurves(X_red,y_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SItF0n2vScbe"
   },
   "source": [
    "### **Output 3** - With All Features - Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7-26UgA9z89"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "# fit predictor and target variable\n",
    "x_smote, y_smote = smote.fit_resample(X, y)\n",
    "X_train_os, X_test_os, y_train_os, y_test_os = train_test_split(x_smote, y_smote, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJT5RNpmHSYK"
   },
   "source": [
    "#### Classification Model Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVjRuQlT90DV"
   },
   "outputs": [],
   "source": [
    "All_OverSampling = Models(X_train_os,y_train_os,X_test_os,y_test_os)\n",
    "All_OverSampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhxEDO5AG6KF"
   },
   "source": [
    "#### ROC Curve Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cCqi_Na90GX"
   },
   "outputs": [],
   "source": [
    "PlotRocCurve(X_train_os,y_train_os,X_test_os,y_test_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVpDXsGQHCGV"
   },
   "source": [
    "#### F Score Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4-lTGJEEdDC"
   },
   "outputs": [],
   "source": [
    "All_OverSampling.plot(x=\"Model Name\",y=[\"f1_macro\",\"f1_micro\",\"f1_weighted\",\"f_beta\"],kind=\"bar\",figsize=(30, 7))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Model Comparision with different f-scores',fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIdRDJoRHJ-u"
   },
   "source": [
    "####  Log Loss Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDfzXrcaEjT3"
   },
   "outputs": [],
   "source": [
    "All_OverSampling.plot(x=\"Model Name\",y=[\"Logloss\"],kind=\"barh\",figsize=(10, 5))\n",
    "plt.xticks(rotation=360)\n",
    "plt.title('Log Loss for Different Models',fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DD_e_vPFGoVq"
   },
   "source": [
    "#### Learning Curve, Scalability and Performance Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf8795HgDWH-"
   },
   "outputs": [],
   "source": [
    "plotperfcurves(x_smote,y_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdCdn4VrFcVO"
   },
   "source": [
    "### **Output 4** - With Reduced Features Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85oAZa5NCVkd"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "# fit predictor and target variable\n",
    "x_smote_red, y_smote_red = smote.fit_resample(X_red, y_red)\n",
    "X_train_red_os, X_test_red_os, y_train_red_os, y_test_red_os = train_test_split(x_smote_red, y_smote_red, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6u7sOsEBHg73"
   },
   "source": [
    "#### Classification Model Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cn7nZ1k90Ji"
   },
   "outputs": [],
   "source": [
    "Red_OverSampling = Models(X_train_red_os,y_train_red_os,X_test_red_os,y_test_red_os)\n",
    "Red_OverSampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRurlbngHn1F"
   },
   "source": [
    "#### F Score Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4J5gdtwEe1R"
   },
   "outputs": [],
   "source": [
    "Red_OverSampling.plot(x=\"Model Name\",y=[\"f1_macro\",\"f1_micro\",\"f1_weighted\",\"f_beta\"],kind=\"bar\",figsize=(30, 7))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Model Comparision with different f-scores',fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzMTftXzHspg"
   },
   "source": [
    "#### Log Loss Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUyGsJ95E2tJ"
   },
   "outputs": [],
   "source": [
    "Red_OverSampling.plot(x=\"Model Name\",y=[\"Logloss\"],kind=\"barh\",figsize=(10, 5))\n",
    "plt.xticks(rotation=360)\n",
    "plt.title('Log Loss for Different Models',fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tqb17a7bGtTy"
   },
   "source": [
    "####  Learning Curve, Scalability and Performance Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeBegb1ADn7C"
   },
   "outputs": [],
   "source": [
    "plotperfcurves(x_smote_red,y_smote_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI7PqaMkJRF3"
   },
   "source": [
    "#### ROC Comparision Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krnxarH6Czl_"
   },
   "outputs": [],
   "source": [
    "PlotRocCurve(X_train_red_os,y_train_red_os,X_test_red_os,y_test_red_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef_5CqZQPowv"
   },
   "source": [
    "### **Output 5** - Reduced Features - Using Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1653yIJDn81b"
   },
   "outputs": [],
   "source": [
    "Features = [ 'Married', 'ApplicantIncome', 'LoanAmount','Credit_History','Education_Not Graduate','Property_Area_Rural', 'Property_Area_Semiurban']\n",
    "X_train_fs= X_train[Features]\n",
    "y_train_fs= y_train\n",
    "X_test_fs = X_test[Features]\n",
    "y_test_fs =y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivlc4u34P2bC"
   },
   "outputs": [],
   "source": [
    "train_enc.Loan_Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgiGI_Lbn_Mq"
   },
   "outputs": [],
   "source": [
    "y_fs = y\n",
    "X_fs = X[Features]\n",
    "X_train_red_fs, X_test_red_fs, y_train_red_fs, y_test_red_fs = train_test_split(X_fs, y_fs, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_AkozABtPt7"
   },
   "source": [
    "#### Learning Curve, Scalability and Performance Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwjor1r0q4ND"
   },
   "outputs": [],
   "source": [
    "plotperfcurves(X_fs,y_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CkG5HXgtTiK"
   },
   "source": [
    "#### ROC Comparision Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JXrj398q8sS"
   },
   "outputs": [],
   "source": [
    "PlotRocCurve(X_train_red_fs,y_train_red_fs,X_test_fs,y_test_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzySjnrYtgY0"
   },
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8_oHnz1rozk"
   },
   "outputs": [],
   "source": [
    "Reduced_Fs = Models(X_train_red_fs,y_train_fs,X_test_fs,y_test_fs)\n",
    "Reduced_Fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKIm7FfCtm4b"
   },
   "source": [
    "#### F Score Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cI3GyK5es3V8"
   },
   "outputs": [],
   "source": [
    "Reduced_Fs.plot(x=\"Model Name\",y=[\"f1_macro\",\"f1_micro\",\"f1_weighted\",\"f_beta\"],kind=\"bar\",figsize=(30, 7))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Model Comparision with different f-scores',fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRgDpvoatu-6"
   },
   "source": [
    "#### Log Loss Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4qFfP0Ks-9a"
   },
   "outputs": [],
   "source": [
    "Reduced_Fs.plot(x=\"Model Name\",y=[\"Logloss\"],kind=\"barh\",figsize=(10, 5))\n",
    "plt.xticks(rotation=360)\n",
    "plt.title('Log Loss for Different Models',fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJM9LGBYvaxR"
   },
   "source": [
    "### **Output 6** - Reduced Features using Feature Selection with Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFBuc4OYvaxb"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "# fit predictor and target variable\n",
    "x_smote_red_fs, y_smote_red_fs = smote.fit_resample(X_fs, y_fs)\n",
    "X_train_red_fs_os, X_test_red_fs_os, y_train_red_fs_os, y_test_red_fs_os = train_test_split(x_smote_red_fs, y_smote_red_fs, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYhAxy-4vaxc"
   },
   "source": [
    "#### Classification Model Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dF_6H5Auvaxc"
   },
   "outputs": [],
   "source": [
    "Red_fs_OverSampling = Models(X_train_red_fs_os,y_train_red_fs_os,X_test_red_fs_os,y_test_red_fs_os)\n",
    "Red_fs_OverSampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwf_Vj4dvaxd"
   },
   "source": [
    "#### F Score Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQr9fc80vaxd"
   },
   "outputs": [],
   "source": [
    "Red_fs_OverSampling.plot(x=\"Model Name\",y=[\"f1_macro\",\"f1_micro\",\"f1_weighted\",\"f_beta\"],kind=\"bar\",figsize=(30, 7))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Model Comparision with different f-scores',fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkRusNT7vaxe"
   },
   "source": [
    "#### Log Loss Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kn9z6aWqvaxe"
   },
   "outputs": [],
   "source": [
    "Red_fs_OverSampling.plot(x=\"Model Name\",y=[\"Logloss\"],kind=\"barh\",figsize=(10, 5))\n",
    "plt.xticks(rotation=360)\n",
    "plt.title('Log Loss for Different Models',fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YV5-t1Ivaxe"
   },
   "source": [
    "####  Learning Curve, Scalability and Performance Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBtaFWutvaxf"
   },
   "outputs": [],
   "source": [
    "plotperfcurves(x_smote_red_fs,y_smote_red_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka862JYBvaxf"
   },
   "source": [
    "#### ROC Comparision Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Eftwb5yvaxf"
   },
   "outputs": [],
   "source": [
    "PlotRocCurve(X_train_red_fs_os,y_train_red_fs_os,X_test_red_fs_os,y_test_red_fs_os)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/8f4051bb3fa7a7812055c023db778e6a"
  },
  "colab": {
   "collapsed_sections": [
    "RKpvzyCOZSrv",
    "dznPTFX6ZSr0",
    "XTb5sb9DZSr1",
    "4j8HKrhgZSr1",
    "kUSfG_tLZSr3",
    "Mh6uw_XyNa29",
    "-2GIIfGjPs_E",
    "NPHiObFAZSr6",
    "FQ-a7ToNN1sc",
    "VcA4QrazN-fp",
    "tfazL65IMlJ-",
    "ipTrdIIYZSr8",
    "oAGL6TRakM34",
    "5jmDxde0XS_K",
    "2SsBM02veJ2Y",
    "-khLY9p8hXzp",
    "6ctJ8zVjZSr9",
    "WSD_TqpmNVig",
    "S1OIOZ4bZSsA",
    "4C3CDIp7ZSr_",
    "jlsIn9ISn7aL",
    "4MDvxWVt8SXj",
    "TXrhMXFAO8hU",
    "_Cl5JeXBi_IO",
    "2tOkCPOVlAd8",
    "2d43UVX9lz3m",
    "igELlXXmwzeX",
    "jCItRtJtq54m",
    "_luNc47QwCRO",
    "QhExplyVrqv3",
    "8WFjEwk7Lp-G",
    "5x4lso8lL22u",
    "tN9xAFrzQSq8",
    "0ZQCq_51NW5U",
    "vJqBBhEwNueh",
    "mlMtU94NZSsD",
    "CcW2Nv_Y5XPb",
    "mC-QTTeh5k7m",
    "lHe6c6Uj5-c_",
    "bB0DuzEf6HkF",
    "hHL3cDc_6RXr",
    "Pq8CCsKZKRPd",
    "QsJuAvnZZSsE",
    "LK3bGkZtKZUZ",
    "yse63xAiGNPI",
    "VNfWKBD1I4vD",
    "1cvlfFo6IzXc",
    "iNEwwiPhIvon",
    "ojHbsZjhIqum",
    "JTr08m5LNMG7",
    "7OBoTy2EIcVO",
    "QKrTGvU4IGOD",
    "o_zZNNUxH96G",
    "gMF6VB-fH4z0",
    "uK_g2vQWGiWb",
    "eJT5RNpmHSYK",
    "LhxEDO5AG6KF",
    "MVpDXsGQHCGV",
    "CIdRDJoRHJ-u",
    "DD_e_vPFGoVq",
    "GdCdn4VrFcVO",
    "6u7sOsEBHg73",
    "fRurlbngHn1F",
    "qzMTftXzHspg",
    "Tqb17a7bGtTy",
    "cI7PqaMkJRF3",
    "J_AkozABtPt7",
    "gzySjnrYtgY0",
    "pKIm7FfCtm4b",
    "AYhAxy-4vaxc",
    "lwf_Vj4dvaxd",
    "KkRusNT7vaxe",
    "1YV5-t1Ivaxe"
   ],
   "name": "P53_Loan_Status_Prediction_Group_5.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gist": {
   "data": {
    "description": "P53_Loan_Status_Prediction_Group5",
    "public": true
   },
   "id": "8f4051bb3fa7a7812055c023db778e6a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
